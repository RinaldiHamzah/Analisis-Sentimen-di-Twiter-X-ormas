# -*- coding: utf-8 -*-
"""Rinaldi Hamzah_5220411342_#ormas.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1eWrjSL0kifYFai2lYoWaZrxpbzLSoYwp
"""

from google.colab import drive
drive.mount('/content/drive')

import pandas as pd
# Membaca file CSV
data = pd.read_csv(
    '/content/drive/MyDrive/Semester7 2026/Pemrosesan Teks Praktik/Ormas.csv',
    encoding='cp1252',
    sep=None,
    engine='python')
print(data.head())

data.shape

data.info

count_data = data[['label']].value_counts().reset_index()
count_data.columns = ['label', 'count']
print(count_data)

import matplotlib.pyplot as plt
plt.figure(figsize=(4,4))
plt.bar(count_data['label'], count_data['count'], color=['#6A5ACD', '#FFA07A', '#20B2AA'])
plt.xlabel("Label")
plt.ylabel("Jumlah")
plt.title("Distribusi Label")
plt.xticks(rotation=0)
plt.show()

"""Word Cloud Data Raw"""

# word cloud
import matplotlib.pyplot as plt
from wordcloud import WordCloud
# Visualisasi Word Cloud
text_all = " ".join(data['full_text'])
wordcloud = WordCloud(width=800, height=400,
                      background_color='white',
                      max_words=100,
                      collocations=False).generate(text_all)
plt.figure(figsize=(10, 6))
plt.imshow(wordcloud, interpolation='bilinear')
plt.axis("off")
plt.title("Word Cloud - Data Raw", fontsize=16)
plt.show()

"""# **ORMAS TEKS REPROSESSING**

Cleaning Teks
"""

import re
def clean_text(text):
    text = str(text).lower() #input berupa string
    text = re.sub(r'http\S+|www\S+|https\S+', '', text) # Hapus URL/link
    text = re.sub(r'@\w+', '', text)  # Hapus mention @username
    text = re.sub(r'#', '', text)  #ormas â†’ ormas
    text = re.sub(r'\d+', '', text) # Hapus angka
    text = re.sub(r'[^a-z\s]', ' ', text) # Hapus tanda baca dan karakter non-huruf
    text = re.sub(r'\brt\b', '', text) # Hapus kata "rt" (retweet)
    text = re.sub(r'\s+', ' ', text).strip()  # Hapus spasi berlebih
    return text
data['clean'] = data['full_text'].apply(clean_text)
print(data[['full_text', 'clean']].head())

"""Normalisasi Teks"""

import nltk
from nltk.tokenize import word_tokenize
# Pastikan tokenizer tersedia
nltk.download('punkt')
nltk.download('punkt_tab')
#Kamus Normalisasi (kata tidak baku â†’ baku)
normalisasi_kamus = {
    'gk': 'tidak', 'ga': 'tidak', 'nggak': 'tidak', 'tdk': 'tidak',
    'dr': 'dari', 'yg': 'yang', 'dgn': 'dengan', 'aja': 'saja',
    'blm': 'belum', 'udh': 'sudah', 'klo': 'kalau', 'kalo': 'kalau',
    'sm': 'sama', 'tp': 'tapi', 'trs': 'terus', 'bgt': 'banget',
    'dg': 'dengan', 'krn': 'karena', 'sy': 'saya', 'gw': 'saya',
    'gue': 'saya', 'loe': 'kamu', 'lu': 'kamu', 'km': 'kamu',
    'nu': 'nahdatul ulama', 'ormas':'organisasi masyarakat',
    'ppi': 'organisasi masyarakat', 'emg': 'memang', 'jg': 'juga',
    'amp': 'sampe', 'pak': 'bapak', 'aja': 'saja', 'lu': 'kamu',
    'jg': 'juga'
}
#Fungsi Normalisasi Kata Tidak Baku
def normalisasi_kata(teks):
    tokens = word_tokenize(str(teks).lower())
    hasil = []
    for k in tokens:
        if k in normalisasi_kamus:
            hasil.append(normalisasi_kamus[k])
        else:
            hasil.append(k)
    return hasil  # hasil dalam bentuk list
data['normal'] = data['clean'].apply(
    lambda x: ' '.join(normalisasi_kata(x)))

print(data[['clean', 'normal']].head())

! pip install sastrawi

""" Lematisasi Teks"""

import re
from Sastrawi.Stemmer.StemmerFactory import StemmerFactory
factory = StemmerFactory()
stemmer_id = factory.create_stemmer()
# Fungsi manual sederhana
def lemmatize_rule_based(kata):
    prefixes = ['meng', 'meny', 'men', 'mem', 'me',
                'peng', 'peny', 'pen', 'pem', 'di',
                'ke', 'se', 'ber', 'ter', 'per']
    suffixes = ['kan', 'an', 'lah', 'nya', 'mu', 'ku']
    # Awalan
    if kata.startswith("meny"):
        kata = "s" + kata[4:]
    elif kata.startswith("mem"):
        kata = kata[3:]
    elif kata.startswith("meng"):
        kata = kata[4:]
    else:
        for p in prefixes:
            if kata.startswith(p) and len(kata) > len(p) + 3:
                kata = kata[len(p):]
                break
    # Akhiran
    for s in suffixes:
        if kata.endswith(s) and len(kata) > len(s) + 3:
            kata = kata[:-len(s)]
            break
    return kata
# Gabungan Manual + Sastrawi
def lemmatize_indonesian_combined(teks):
    teks = str(teks).lower()
    tokens = re.findall(r'\b[a-zA-Z]+\b', teks)
    hasil_manual = [lemmatize_rule_based(k) for k in tokens]
    teks_manual = ' '.join(hasil_manual)
    # Lemmatization tambahan dengan Sastrawi
    hasil_sastrawi = stemmer_id.stem(teks_manual)
    return hasil_sastrawi
# Aplikasi ke DataFrame
data['lemma'] = data['normal'].apply(lemmatize_indonesian_combined)

print(data[['normal', 'lemma']].head())

"""**Steaming Teks**"""

from Sastrawi.Stemmer.StemmerFactory import StemmerFactory
from nltk.stem import PorterStemmer
# Inisialisasi Stemmer
factory = StemmerFactory()
stemmer_id = factory.create_stemmer()
stemmer_en = PorterStemmer()
#  Deteksi Bahasa
def detect_language(text):
    eng = re.findall(r'\b(the|is|are|and|for|to|of)\b', text.lower())
    indo = re.findall(r'\b(yang|dan|dari|ke|di|itu|dengan)\b', text.lower())
    return 'en' if len(eng) > len(indo) else 'id'
#Fungsi dengan log perubahan
def bilingual_stem_verbose(text):
    lang = detect_language(text)
    tokens = text.split()
    results = []

    if lang == 'en':
        for t in tokens:
            stemmed = stemmer_en.stem(t)
            results.append((t, stemmed))
    else:
        for t in tokens:
            stemmed = stemmer_id.stem(t)
            results.append((t, stemmed))
    # Print log perubahan per kata
    print(f"\nðŸ”¤ Input: {text}")
    print(f"ðŸŒ Language detected: {lang.upper()}")
    for original, stemmed in results:
        if original != stemmed:
            print(f"   {original:15} âžœ {stemmed}")
        else:
            print(f"   {original:15} (no change)")
    # Gabungkan hasil akhir
    return ' '.join([s for _, s in results])
# Terapkan fungsi dan tampilkan log
data["stem"] = data["lemma"].apply(bilingual_stem_verbose)

print(data[['lemma', 'stem']].head())

"""**Stopword Removal Teks**"""

import nltk
from nltk.corpus import stopwords
from Sastrawi.StopWordRemover.StopWordRemoverFactory import StopWordRemoverFactory
#Download resource penting
nltk.download('stopwords')
nltk.download('punkt')
#Inisialisasi stopwords bawaan
factory = StopWordRemoverFactory()
stopwords_id = set(factory.get_stop_words())
stopwords_en = set(stopwords.words('english'))
#Stopword manual (tambahan sosial media)
stopwords_manual = {
    'yg', 'gk', 'ga', 'nggak', 'tdk', 'aja', 'nih', 'dong', 'deh', 'sih', 'dih',
    'ya', 'kok', 'loh', 'wkwk', 'haha', 'hehe', 'anjir', 'anjay', 'wkwkwkwwkwk',
    'bro', 'sis', 'gan', 'bang', 'gua', 'gue', 'lu', 'loe', 'gw', 'km', 'sy',
    'nih', 'nya', 'lah', 'dong', 'banget', 'bgt', 'rt', 'via', 'sm', 'dr',
    'ttp', 'pdhl', 'gitu', 'loh', 'wkwkwk', 'hahaha', 'klo', 'kalo', 'tp',
    'apa', 'kan', 'si', 'jang', 'jadi', 'jd', 'ang', 'deng', 'mah', 'tuh', 'kek',
    'bahk', 'juga', 'ndiri', 'gak', 'napa', 'prem', 'erintah', 'padahal', 'mana',
    'kyknya', 'gibr', 'dah', 'utk', 'tak', 'bnyk', 'g', 't', 'i', 'ang'
}
stopwords_id.update(stopwords_manual)
#Kamus normalisasi slang (biar cocok)
normalisasi_kamus = {
    'gk': 'tidak', 'ga': 'tidak', 'nggak': 'tidak', 'tdk': 'tidak', 'kntl': 'keren',
    'yg': 'yang', 'aja': 'saja', 'bgt': 'banget', 'udh': 'sudah',
    'klo': 'kalau', 'kalo': 'kalau', 'sm': 'sama', 'dr': 'dari',
    'tp': 'tapi', 'sy': 'saya', 'gue': 'saya', 'gua': 'saya', 'loe': 'kamu', 'lu': 'kamu',
    'ponpres': 'pondok pesantren', 'mbg': 'makan bergiji gratis'
}
#Deteksi bahasa sederhana ===
def detect_language(text):
    eng = re.findall(r'\b(the|is|are|of|and|to|for)\b', text.lower())
    indo = re.findall(r'\b(yang|dan|di|ke|dari|untuk|itu|dengan)\b', text.lower())
    return 'en' if len(eng) > len(indo) else 'id'
#Fungsi Stopword Removal + Normalisasi
def bilingual_stopword_removal(text):
    text = str(text).lower()
    tokens = nltk.word_tokenize(text)
    lang = detect_language(text)
    normalized_tokens = []
    for w in tokens:
        # normalisasi slang
        if w in normalisasi_kamus:
            w = normalisasi_kamus[w]
        normalized_tokens.append(w)
    if lang == 'en':
        filtered = [w for w in normalized_tokens if w not in stopwords_en]
    else:
        filtered = [w for w in normalized_tokens if w not in stopwords_id]
    return ' '.join(filtered)
#Terapkan ke DataFrame
data['stop'] = data['stem'].apply(bilingual_stopword_removal)

print(data[['stem', 'stop']].head())

"""Word Cloud"""

# word cloud
import matplotlib.pyplot as plt
from wordcloud import WordCloud
# Visualisasi Word Cloud
text_all = " ".join(data['stop'])
wordcloud = WordCloud(width=800, height=400,
                      background_color='white',
                      max_words=100,
                      collocations=False).generate(text_all)
plt.figure(figsize=(10, 6))
plt.imshow(wordcloud, interpolation='bilinear')
plt.axis("off")
plt.title("Word Cloud Hasil Preprosesing", fontsize=16)
plt.show()

"""**Tokenisasi Teks**"""

! pip install tensorflow

from tensorflow.keras.preprocessing.text import Tokenizer
tokenizer = Tokenizer(num_words=10000, oov_token='<OOV>')
tokenizer.fit_on_texts(data['stop'])
data['token'] = tokenizer.texts_to_sequences(data['stop'])
print(data[['stop', 'token']].head())

tempArr = []
tokenizer.word_index

for key in tokenizer.word_index:
  tempArr.append(key)

numTextTokenized = []
for i in data['token']:
  tempArr_2 = []
  for j in i:
    tempArr_2.append(j)
  numTextTokenized.append(tempArr_2)

for i in range(len(numTextTokenized)):
  for j in range(len(numTextTokenized[i])):
    numTextTokenized[i][j] = tempArr[numTextTokenized[i][j] - 1]
data['tokening'] = numTextTokenized

print(data[['stop', 'tokening']].head())

# data.to_csv('Ormas_preprosesing.csv', index=False)

"""## TF-IDF FRECUENCY"""

# Melihat kembali baris yang telah di tokenisasi
for i, tokens in enumerate(data['tokening']):
    print(f"Review {i+1}:")
    print(tokens, "\n")

index = 0
for i in data['tokening']:
  if index == 5:
    break
  print(type(i))
  index += 1

"""**Perhitungan TF**"""

def calc_TF(document):
    # Step 1: Count term occurrences
    tf_dict = {}
    for term in document:
        if term in tf_dict:
            tf_dict[term] += 1
        else:
            tf_dict[term] = 1
    # Step 2: Normalize by the total number of terms (TF calculation)
    for term in tf_dict:
        tf_dict[term] = tf_dict[term] / len(document)
    return tf_dict
# Apply TF calculation to each list of tokens in 'tekslist'
data["tf"] = data["tokening"].apply(calc_TF)
# Display the first few TF dictionaries
data["tf"].head()

#Displaying a Documentâ€™s TF Values
#Prints all terms and their Term Frequency (TF) values for a specific document (here, document index = 1)
index = 0
print("%20s" % "term", "\t", "tf\n")
for key in data["tf"][index]:
    print('%20s' % key, "\t", data["tf"][index][key])

"""**Perhitungan IDF**"""

#Calculating Document Frequency (DF)
def calc_DF(tfDict):
    count_DF = {}
    for document in tfDict:
        for term in document:
            if term in count_DF:
                count_DF[term] += 1
            else:
                count_DF[term] = 1
    return count_DF

DF = calc_DF(data["tf"])
DF

#Calculating Inverse Document Frequency (IDF)
import numpy as np
n_document = len(data)
def calc_IDF(n_document, DF):
    IDF_dict = {}
    for term in DF:
        IDF_dict[term] = np.log10(n_document / (DF[term] + 1))
    return IDF_dict
IDF = calc_IDF(n_document, DF)

selected_terms = ['bubar', 'ulama', 'hukum']
idf_selected = {term: IDF.get(term, 0) for term in selected_terms}
print("Nilai IDF untuk kata ['bubar', 'ulama', 'hukum']:")
print(idf_selected)

"""Hasil perhitungan TF_IDF"""

#Calculating TF-IDF
#Purpose:
#Computes the TF-IDF value for each term in each document by multiplying TF and IDF.
def calc_TF_IDF(TF, IDF):
    TF_IDF_dict = {}
    for key in TF:
        if key in IDF:  # add safety check
            TF_IDF_dict[key] = TF[key] * IDF[key]
    return TF_IDF_dict
# Apply properly
data["tf-idf"] = data["tf"].apply(lambda x: calc_TF_IDF(x, IDF))

# Tampilkan nilai TF dan TF-IDF untuk dokumen ke-1
index = 0
print(f'{"Term":<20}\t{"TF":<10}\t{"TF-IDF":<20}')
print('-' * 60)
for term, tfidf_val in data["tf-idf"][index].items():
    tf_val = data["tf"][index].get(term, 0)
    print(f'{term:<20}\t{tf_val:<10.4f}\t{tfidf_val:<20.6f}')

"""Matrik TF-IDF Vectorizer"""

# Membuat matriks TF-IDF
# Urutkan kata berdasarkan Document Frequency (descending)
sorted_DF = sorted(DF.items(), key=lambda kv: kv[1], reverse=True)[:30]
unique_terms = [term for term, _ in sorted_DF]
# Fungsi untuk mengonversi TF-IDF dict ke vektor
def calc_TF_IDF_Vec(tfidf_dict):
    vector = [0.0] * len(unique_terms)
    for i, term in enumerate(unique_terms):
        vector[i] = tfidf_dict.get(term, 0.0)
    return vector
# Terapkan fungsi ke setiap dokumen
data["tf_idf_vector"] = data["tf-idf"].apply(calc_TF_IDF_Vec)
# Cek hasil
print("Tampilan baris pertama dari matriks tf_idf_vector:\n")
print(data["tf_idf_vector"].iloc[1])
print("\nUkuran vektor (jumlah fitur):", len(data["tf_idf_vector"].iloc[0]))

"""Menampilkan hasil ke dalam list"""

# Menampilkan top 30 term berdasarkan total TF-IDF
# Konversi kolom TF-IDF vector menjadi array 2D
TF_IDF_Vec_List = np.array(data["tf_idf_vector"].to_list())
# Hitung jumlah TF-IDF setiap term di seluruh dokumen
term_sums = TF_IDF_Vec_List.sum(axis=0)
# Buat DataFrame untuk peringkat term
ranking = pd.DataFrame({
    'Term': unique_terms,
    'Rank': term_sums
})
# Urutkan dari nilai tertinggi ke terendah
ranking = ranking.sort_values(by='Rank', ascending=False).reset_index(drop=True)
# Tampilkan top 30 term
print("Top 30 terms berdasarkan total TF-IDF:")
print(ranking.head(100))

#Analisis kata penting per dokumen
for i, tfidf_vector in enumerate(TF_IDF_Vec_List[:10]):  # contoh 5 dokumen pertama
    top_indices = tfidf_vector.argsort()[-5:][::-1]  # ambil 5 term tertinggi
    top_terms = [unique_terms[idx] for idx in top_indices]
    print(f"Dokumen {i}: {top_terms}")

# data = pd.read_csv('/content/drive/MyDrive/Semester7 2026/Pemrosesan Teks Praktik/Ormas_preprosesing.csv')
# data.head()
# print(data[['stop', 'token', 'tokening']].head())

"""# Klasifikasi Algoritma Model

# Klasifikasi langsung Menggunakan nilai Vector TF-IDF
"""

from sklearn.model_selection import train_test_split

# TF-IDF matrix
X = np.array(data["tf_idf_vector"].to_list())

# Jika kamu punya label (misal: kategori teks)

y = data["label"]

from sklearn.naive_bayes import MultinomialNB
from sklearn.metrics import accuracy_score, classification_report
# Bagi data jadi train-test
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
# Buat dan latih model
nb_model = MultinomialNB()
nb_model.fit(X_train, y_train)
# Prediksi
y_pred = nb_model.predict(X_test)
# Evaluasi hasil
print("Akurasi:", accuracy_score(y_test, y_pred))
print("Laporan klasifikasi:\n", classification_report(y_test, y_pred))

"""# Klasifikasi Mengguanakan Naive Bayes 3 Label dan Smote"""

X = data["stop"]
y = data["label"]

print("Distribusi Label Awal:")
print(y.value_counts())

from sklearn.feature_extraction.text import TfidfVectorizer

tfidf = TfidfVectorizer(
    ngram_range=(1,2),
    min_df=2,
    max_features=20000,
    lowercase=True
)

X_tfidf = tfidf.fit_transform(X)

from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(
    X_tfidf, y,
    test_size=0.2,
    stratify=y,
    random_state=42
)

from imblearn.over_sampling import SMOTE

smote = SMOTE(random_state=42)
X_train_smote, y_train_smote = smote.fit_resample(X_train, y_train)

print("\nDistribusi Label Setelah SMOTE:")
print(pd.Series(y_train_smote).value_counts())

from sklearn.naive_bayes import MultinomialNB
from sklearn.model_selection import GridSearchCV

param_grid = {
    'alpha': [0.1, 0.3, 0.5, 0.7, 1.0]
}

grid = GridSearchCV(
    estimator=MultinomialNB(),
    param_grid=param_grid,
    cv=5,
    scoring='accuracy',
    n_jobs=-1,
    verbose=2
)

grid.fit(X_train_smote, y_train_smote)

print("\nAlpha terbaik:", grid.best_params_)
print("Akurasi rata-rata CV:", grid.best_score_)

best_alpha = grid.best_params_['alpha']
best_nb = MultinomialNB(alpha=best_alpha)

best_nb.fit(X_train_smote, y_train_smote)

from sklearn.metrics import accuracy_score, classification_report

y_pred = best_nb.predict(X_test)

print("\nAkurasi Test Set:", accuracy_score(y_test, y_pred))
print("\nClassification Report:")
print(classification_report(y_test, y_pred))

"""# Suport Vector Machine(SVM) 3 Label dan Smote"""

from sklearn.feature_extraction.text import TfidfVectorizer

tfidf = TfidfVectorizer(
    ngram_range=(1,2),   # unigram + bigram
    max_features=20000,
    min_df=2,
    lowercase=True
)

X_tfidf = tfidf.fit_transform(X)

from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(
    X_tfidf, y,
    test_size=0.2,
    stratify=y,
    random_state=42
)

from imblearn.over_sampling import SMOTE

smote = SMOTE(random_state=42, k_neighbors=3)
X_train_smote, y_train_smote = smote.fit_resample(X_train, y_train)

print("\nDistribusi Label Setelah SMOTE:")
print(pd.Series(y_train_smote).value_counts())

param_grid = {
    "C": [0.1, 0.5, 1, 2, 3],
    "loss": ["hinge", "squared_hinge"],
    "class_weight": [None, "balanced"]
}

grid = GridSearchCV(
    LinearSVC(max_iter=5000),
    param_grid,
    cv=5,
    scoring="accuracy",
    n_jobs=-1
)
grid.fit(X_train_smote, y_train_smote)

print("\nParameter Terbaik:", grid.best_params_)
print("Akurasi CV Terbaik:", grid.best_score_)

best_params = grid.best_params_

best_svm = LinearSVC(
    C=best_params["C"],
    loss=best_params["loss"],
    class_weight=best_params["class_weight"],
    max_iter=5000
)

best_svm.fit(X_train_smote, y_train_smote)

y_pred = best_svm.predict(X_test)

print("Akurasi:", accuracy_score(y_test, y_pred))
print("\nClassification Report:")
print(classification_report(y_test, y_pred))

"""Kesimpulan Hasil Teks Prosesing dan Analisis Model

1.   Data Undestanding
    Pada bagian data ini sudah mengalami sedikit pemasalahan dikarenakan data tidak memiliki pola pembahasan yang tertuju dengan hasil data scraping tautu #ormas. Data sangatlah tidak beraturan.
2.   Labeling Data Secara Manual
    Dikarenakan pada saat labeling di gunakan 3 bentuk label yaitu pisitive,negative dan neutral terjadi sedikit permasalahan karena sebagian besar data mengarah ke negative dan sangat susah untuk meberiakan label ke positif dan pada akhirnya di labeling menjadi neutral.
3.   TF-IDF
    Hasil yang di dapatkan setelah melakukan tf-idf terbilang data sangatlah bersih dari 30 kata dengan bobot tertingggi merupakan data yang sudah bersih.
4.   Clasifikasi Model
    Klasifikasi menggunakan 2 model yaitu naive bayes dan svm dengan akurasi hasil yang didapatkan secara berturut turut 68% dan 65%. Hasil ini terbilang sangatlah rendah namun hasil ini juga terbilang bagus karena bayak data yang di labeling sangat susah untuk membedakanya.

# Naive Bayes Menggunakan 2 Label dan Smote
"""

data_binary = data[data['label'] != 'neutral'].copy()

print("Jumlah data sebelum:", len(data))
print("Jumlah data setelah drop neutral:", len(data_binary))
print("\nDistribusi Label Baru:")
print(data_binary['label'].value_counts())

X = data_binary['stop']
y = data_binary['label']

from sklearn.feature_extraction.text import TfidfVectorizer

tfidf = TfidfVectorizer(
    ngram_range=(1,2),
    min_df=2,
    max_features=20000,
    lowercase=True
)

X_tfidf = tfidf.fit_transform(X)

from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(
    X_tfidf, y,
    test_size=0.2,
    random_state=42,
    stratify=y
)

from imblearn.over_sampling import SMOTE
import pandas as pd

smote = SMOTE(random_state=42)
X_train_smote, y_train_smote = smote.fit_resample(X_train, y_train)

print("\nDistribusi label setelah SMOTE:")
print(pd.Series(y_train_smote).value_counts())

from sklearn.naive_bayes import MultinomialNB
from sklearn.metrics import accuracy_score, classification_report

nb_model = MultinomialNB()
nb_model.fit(X_train_smote, y_train_smote)

y_pred = nb_model.predict(X_test)

print("\nAkurasi:", accuracy_score(y_test, y_pred))
print("\nClassification Report:")
print(classification_report(y_test, y_pred))

"""# SVM Menggunakan 2 Label dan Smote"""

# Drop semua data dengan label 'neutral'
data_binary = data[data['label'] != 'neutral'].copy()

print("Jumlah data sebelum:", len(data))
print("Jumlah data setelah drop neutral:", len(data_binary))

# Cek distribusi label baru
print("\nDistribusi Label Baru:")
print(data_binary['label'].value_counts())

X = data_binary['stop']
y = data_binary['label']

from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import TfidfVectorizer

# TF-IDF
tfidf = TfidfVectorizer(
    ngram_range=(1,2),
    min_df=2,
    max_features=20000
)
X_tfidf = tfidf.fit_transform(X)

# Split
X_train, X_test, y_train, y_test = train_test_split(
    X_tfidf, y,
    test_size=0.2,
    random_state=42,
    stratify=y
)

from imblearn.over_sampling import SMOTE

smote = SMOTE(random_state=42)
X_train_smote, y_train_smote = smote.fit_resample(X_train, y_train)

print("\nDistribusi label setelah SMOTE:")
print(pd.Series(y_train_smote).value_counts())

param_grid = {
    'C': [0.1, 0.3, 0.5, 1, 2, 3],
    'loss': ['hinge', 'squared_hinge'],
    'class_weight': [None, 'balanced']
}

svm = LinearSVC()

grid = GridSearchCV(
    estimator=svm,
    param_grid=param_grid,
    cv=5,
    scoring='accuracy',
    n_jobs=-1,
    verbose=2
)
grid.fit(X_train_smote, y_train_smote)

print("Parameter terbaik:", grid.best_params_)
print("Akurasi rata-rata CV:", grid.best_score_)

best_svm = LinearSVC(
    C=grid.best_params_['C'],
    loss=grid.best_params_['loss'],
    class_weight=grid.best_params_['class_weight']
)

best_svm.fit(X_train_smote, y_train_smote)

y_pred = best_svm.predict(X_test)

print("\nAkurasi Test Set:", accuracy_score(y_test, y_pred))
print("\nClassification Report:")
print(classification_report(y_test, y_pred))

"""# **BERT / IndoBERT**

Bagian	Optimasi
Representasi Teks,	IndoBERT (lebih bagus untuk Bahasa Indonesia dibanding TF-IDF)

Imbalanced Dataset,	class_weight='balanced' untuk mengatasi bias

Tokenizer,	BERT tokenizer (better semantic capture)

Loss Function,	CrossEntropyLoss dengan class_weight

Training, Strategy	AdamW, LR 2e-5, eval per epoch

Metrics,	Weighted F1, Accuracy, Precision, Recall

"""

from sklearn.model_selection import train_test_split
from sklearn.utils.class_weight import compute_class_weight
from sklearn.preprocessing import LabelEncoder
from datasets import Dataset
import torch

from transformers import (
    BertTokenizerFast,
    BertForSequenceClassification,
    TrainingArguments,
    Trainer
)

from sklearn.metrics import accuracy_score, precision_recall_fscore_support

df = data.dropna(subset=["stop", "label"])

# Encode label
le = LabelEncoder()
df["label"] = le.fit_transform(df["label"])

# Train-val split
train_df, val_df = train_test_split(df, test_size=0.2, stratify=df["label"], random_state=42)

train_ds = Dataset.from_pandas(train_df)
val_ds   = Dataset.from_pandas(val_df)

MODEL_NAME = "indobenchmark/indobert-base-p1"
tokenizer = BertTokenizerFast.from_pretrained(MODEL_NAME)

# Tokenizing function
def tokenize(batch):
    return tokenizer(
        batch["stop"],
        padding="max_length",
        truncation=True,
        max_length=128
    )

train_ds = train_ds.map(tokenize, batched=True)
val_ds = val_ds.map(tokenize, batched=True)

# Remove unused columns
train_ds = train_ds.remove_columns(["stop", "__index_level_0__"])
val_ds   = val_ds.remove_columns(["stop", "__index_level_0__"])

train_ds.set_format("torch")
val_ds.set_format("torch")

class_names = np.unique(train_df["label"])
weights = compute_class_weight(
    class_weight="balanced",
    classes=class_names,
    y=train_df["label"]
)

class_weights_tensor = torch.tensor(weights, dtype=torch.float)

class WeightedBERT(torch.nn.Module):
    def __init__(self):
        super().__init__()
        self.model = BertForSequenceClassification.from_pretrained(
            MODEL_NAME,
            num_labels=len(class_names)
        )
        self.loss_fct = torch.nn.CrossEntropyLoss(weight=class_weights_tensor)

    def forward(self, input_ids=None, attention_mask=None, labels=None):
        outputs = self.model(
            input_ids=input_ids,
            attention_mask=attention_mask,
            labels=None
        )
        logits = outputs.logits

        loss = None
        if labels is not None:
            loss = self.loss_fct(logits, labels)

        return {"loss": loss, "logits": logits}

model = WeightedBERT()

def compute_metrics(pred):
    labels = pred.label_ids
    preds = np.argmax(pred.predictions, axis=1)

    acc = accuracy_score(labels, preds)
    p, r, f1, _ = precision_recall_fscore_support(labels, preds, average="weighted")

    return {
        "accuracy": acc,
        "precision": p,
        "recall": r,
        "f1": f1
    }

training_args = TrainingArguments(
    output_dir="./bert-indo-model",
    eval_strategy="epoch",
    save_strategy="epoch",
    learning_rate=2e-5,
    per_device_train_batch_size=16,
    per_device_eval_batch_size=16,
    num_train_epochs=4,
    weight_decay=0.01,
    logging_steps=50,
)

# @title
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_ds,
    eval_dataset=val_ds,
    tokenizer=tokenizer,
    compute_metrics=compute_metrics
)

trainer.train()

eval_result = trainer.evaluate()
print("Evaluation:", eval_result)